# ğŸš¢ Titanic Survival Prediction (Kaggle)

This is my solution for the classic [Titanic Kaggle competition](https://www.kaggle.com/c/titanic).  
In this project, I built a machine learning pipeline to predict which passengers survived the Titanic disaster.

---

## ğŸ“Š Final Public Score: **0.76794**
Model: **XGBoost Classifier** with tuned hyperparameters  
Validation Accuracy: **83.2%**

---

## ğŸ› ï¸ Tools Used
- Python
- Pandas, NumPy
- Scikit-learn
- XGBoost
- Matplotlib (optional, for EDA)
- VSCode

---

## ğŸ§ª Workflow
1. ğŸ“¥ Load & clean dataset
2. ğŸ” Exploratory Data Analysis (EDA)
3. ğŸ§¼ Feature Engineering
4. ğŸ§  Model training & tuning (GridSearchCV)
5. ğŸ“ Evaluation & Submission

---

## ğŸ¯ Next Steps (optional ideas)
- Add new features: `Title`, `CabinDeck`, `IsAlone`
- Try ensemble models (VotingClassifier)
- Plot feature importance from XGBoost

---

Made by Sikka Milladia (https://www.kaggle.com/sikkamilladia)  
